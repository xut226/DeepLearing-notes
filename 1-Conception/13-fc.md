在整个卷积神经网络中起到“分类器”的作用。

全连接的核心操作就是矩阵向量乘积

![](https://www.zhihu.com/equation?tex=y+%3D+Wx "y = Wx")

本质就是由一个特征空间线性变换到另一个特征空间。目标空间的任一维——也就是隐层的一个 cell——都认为会受到源空间的每一维的影响。不考虑严谨，可以说，目标向量是源向量的加权和。

全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。处理过程：用 feature map 直接表示属于某个类的 confidence map，比如有10个类，就在最后输出10个 feature map，每个feature map中的值加起来求平均值，然后把得到的这些平均值直接作为属于某个类别的 confidence value，再输入softmax中分类。参考文献：Network In Network

FC可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在ImageNet上预训练得到的模型为![](https://www.zhihu.com/equation?tex=\mathcal{M} "\mathcal{M}") ，则ImageNet可视为源域（迁移学习中的source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比ImageNet，目标域图像不是物体为中心的图像，而是风景照，见下图），不含FC的网络微调后的结果要差于含FC的网络。因此FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。参考文献：[In Defense of Fully Connected Layers in Visual Representation Transfer](https://link.zhihu.com/?target=https%3A//cs.nju.edu.cn/wujx/paper/PCM2017_FC.pdf)（[https://cs.nju.edu.cn/wujx/paper/PCM2017\_FC.pdf）](https://cs.nju.edu.cn/wujx/paper/PCM2017_FC.pdf）)



