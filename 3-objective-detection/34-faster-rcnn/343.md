### training

1.**数据**

对与每张图片，需要进行如下数据处理：

图片进行缩放，使得长边小于等于1000，短边小于等于600（至少有一个等于）。

对相应的bounding boxes 也也进行同等尺度的缩放。

对于Caffe 的VGG16 预训练模型，需要图片位于0-255，BGR格式，并减去一个均值，使得图片像素的均值为0。

最后返回四个值供模型训练：

images ： 3×H×W ，BGR三通道，宽W，高H

bboxes： 4×K , K个bounding boxes，每个bounding box的左上角和右下角的座标，形如（Y\_min,X\_min, Y\_max,X\_max）,第Y行，第X列。

labels：K， 对应K个bounding boxes的label（对于VOC取值范围为\[0-19\]）

scale: 缩放的倍数, 原图H' ×W'被resize到了HxW（scale=H/H' ）

需要注意的是，目前大多数Faster R-CNN实现都只支持batch-size=1的训练

**2 Extractor**

Extractor使用的是预训练好的模型提取图片的特征。论文中主要使用的是Caffe的预训练模型VGG16。修改如下图所示：为了节省显存，前四层卷积层的学习率设为0。Conv5\_3的输出作为图片特征（feature）。conv5\_3相比于输入，下采样了16倍，也就是说输入的图片尺寸为3×H×W，那么feature的尺寸就是C×\(H/16\)×\(W/16\)。VGG最后的三层全连接层的前两层，一般用来初始化RoIHead的部分参数，这个我们稍后再讲。总之，一张图片，经过extractor之后，会得到一个C×\(H/16\)×\(W/16\)的feature map。

**3 RPN**

Faster R-CNN最突出的贡献就在于提出了Region Proposal Network（RPN）代替了Selective Search，从而将候选区域提取的时间开销几乎降为0（2s -&gt; 0.01s）。

3.1 **Anchor**

在RPN中，作者提出了anchor。Anchor是大小和尺寸固定的候选框。论文中用到的anchor有三种尺寸和三种比例，如下图所示，三种尺寸分别是小（蓝128）中（红256）大（绿512），三个比例分别是1:1，1:2，2:1。3×3的组合总共有9种anchor。

然后用这9种anchor在特征图（feature）左右上下移动，每一个特征图上的点都有9个anchor，最终生成了 \(H/16\)× \(W/16\)×9个anchor. 对于一个512×62×37的feature map，有 62×37×9~ 20000个anchor。 也就是对一张图片，有20000个左右的anchor。这种做法很像是暴力穷举，20000多个anchor，哪怕是蒙也能够把绝大多数的ground truth bounding boxes蒙中。

**3.2 训练RPN**

RPN的总体架构如下图所示：

![](/assets/3.4.3 RPN_training.png)  


reference:

[https://www.jianshu.com/p/9da1f0756813](https://www.jianshu.com/p/9da1f0756813)

