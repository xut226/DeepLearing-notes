### training

1.**数据**

对与每张图片，需要进行如下数据处理：

图片进行缩放，使得长边小于等于1000，短边小于等于600（至少有一个等于）。

对相应的bounding boxes 也也进行同等尺度的缩放。

对于Caffe 的VGG16 预训练模型，需要图片位于0-255，BGR格式，并减去一个均值，使得图片像素的均值为0。

最后返回四个值供模型训练：

images ： 3×H×W ，BGR三通道，宽W，高H

bboxes： 4×K , K个bounding boxes，每个bounding box的左上角和右下角的座标，形如（Y\_min,X\_min, Y\_max,X\_max）,第Y行，第X列。

labels：K， 对应K个bounding boxes的label（对于VOC取值范围为\[0-19\]）

scale: 缩放的倍数, 原图H' ×W'被resize到了HxW（scale=H/H' ）

需要注意的是，目前大多数Faster R-CNN实现都只支持batch-size=1的训练

**2 Extractor**

Extractor使用的是预训练好的模型提取图片的特征。论文中主要使用的是Caffe的预训练模型VGG16。修改如下图所示：为了节省显存，前四层卷积层的学习率设为0。Conv5\_3的输出作为图片特征（feature）。conv5\_3相比于输入，下采样了16倍，也就是说输入的图片尺寸为3×H×W，那么feature的尺寸就是C×\(H/16\)×\(W/16\)。VGG最后的三层全连接层的前两层，一般用来初始化RoIHead的部分参数，这个我们稍后再讲。总之，一张图片，经过extractor之后，会得到一个C×\(H/16\)×\(W/16\)的feature map。

**3 RPN**

Faster R-CNN最突出的贡献就在于提出了Region Proposal Network（RPN）代替了Selective Search，从而将候选区域提取的时间开销几乎降为0（2s -&gt; 0.01s）。

3.1 **Anchor**

在RPN中，作者提出了anchor。Anchor是大小和尺寸固定的候选框。论文中用到的anchor有三种尺寸和三种比例，如下图所示，三种尺寸分别是小（蓝128）中（红256）大（绿512），三个比例分别是1:1，1:2，2:1。3×3的组合总共有9种anchor。

然后用这9种anchor在特征图（feature）左右上下移动，每一个特征图上的点都有9个anchor，最终生成了 \(H/16\)× \(W/16\)×9个anchor. 对于一个512×62×37的feature map，有 62×37×9~ 20000个anchor。 也就是对一张图片，有20000个左右的anchor。这种做法很像是暴力穷举，20000多个anchor，哪怕是蒙也能够把绝大多数的ground truth bounding boxes蒙中。

**3.2 训练RPN**

RPN的总体架构如下图所示：

![](/assets/3.4.3 RPN_training.png)

RPN架构

anchor的数量和feature map相关，不同的feature map对应的anchor数量也不一样。RPN在Extractor输出的feature maps的基础之上，先增加了一个卷积（用来语义空间转换？），然后利用两个1x1的卷积分别进行二分类（是否为正样本）和位置回归。进行分类的卷积核通道数为9×2（9个anchor，每个anchor二分类，使用交叉熵损失），进行回归的卷积核通道数为9×4（9个anchor，每个anchor有4个位置参数）。RPN是一个全卷积网络（fully convolutional network），这样对输入图片的尺寸就没有要求了。

接下来RPN做的事情就是利用（AnchorTargetCreator）将20000多个候选的anchor选出256个anchor进行分类和回归位置。选择过程如下：

对于每一个ground truth bounding box \(gt\_bbox\)，选择和它重叠度（IoU）最高的一个anchor作为正样本

对于剩下的anchor，从中选择和任意一个gt\_bbox重叠度超过0.7的anchor，作为正样本，正样本的数目不超过128个。

随机选择和gt\_bbox重叠度小于0.3的anchor作为负样本。负样本和正样本的总数为256。

对于每个anchor, gt\_label 要么为1（前景），要么为0（背景），而gt\_loc则是由4个位置参数\(tx,ty,tw,th\)组成，这样比直接回归座标更好。

计算分类损失用的是交叉熵损失，而计算回归损失用的是Smooth\_l1\_loss. 在计算回归损失的时候，只计算正样本（前景）的损失，不计算负样本的位置损失。

**3.3 RPN生成RoIs**

RPN在自身训练的同时，还会提供RoIs（region of interests）给Fast RCNN（RoIHead）作为训练样本。RPN生成RoIs的过程\(ProposalCreator\)如下：

对于每张图片，利用它的feature map， 计算 \(H/16\)× \(W/16\)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。

选取概率较大的12000个anchor

利用回归的位置参数，修正这12000个anchor的位置，得到RoIs

利用非极大值（\(Non-maximum suppression, NMS）抑制，选出概率最大的2000个RoIs

注意：在inference的时候，为了提高处理速度，12000和2000分别变为6000和300.

注意：这部分的操作不需要进行反向传播，因此可以利用numpy/tensor实现。

RPN的输出：RoIs（形如2000×4或者300×4的tensor）



**4 RoIHead/Fast R-CNN**

RPN只是给出了2000个候选框，RoI Head在给出的2000候选框之上继续进行分类和位置参数的回归。

**4.1 网络结构**

![](/assets/3.4.3 training_fastrcnn.png)



### Reference:

[https://www.jianshu.com/p/9da1f0756813](https://www.jianshu.com/p/9da1f0756813)

